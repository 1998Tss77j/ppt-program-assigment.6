# -*- coding: utf-8 -*-
"""PPT program assignment=6

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rOi_rvGXxlaAcKZ10RLU2NoFnkWNR_mo

1. Data Ingestion Pipeline:

a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.
   b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.
   c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing.

ANS=
a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.

Answer:
To design a data ingestion pipeline that collects and stores data from various sources, we can follow a modular and scalable approach. Here's a high-level design for such a pipeline:

Identify data sources: Determine the types of sources we'll be collecting data from, such as databases, APIs, and streaming platforms.

Extract data: Implement extraction methods specific to each data source. For databases, we can use query-based approaches or database connectors. For APIs, we can make HTTP requests to fetch data. For streaming platforms, we may need to subscribe to streams or use message queues.

Transform and cleanse data: Perform any necessary data transformation and cleansing operations to ensure data consistency and quality. This step may involve data normalization, deduplication, filtering, or aggregation.

Store data: Choose an appropriate storage solution based on the characteristics of the data, such as relational databases, NoSQL databases, data lakes, or cloud storage. Store the processed data in the selected storage system.

Monitor and validate: Implement monitoring mechanisms to ensure the pipeline's health and data accuracy. Set up alerts for any failures or anomalies detected during the ingestion process.

Here's a simplified code snippet in Python that demonstrates the extraction of data from a database and storing it in a relational database:
"""

import psycopg2

# Connect to the source database
source_conn = psycopg2.connect(database="source_db", user="username", password="password", host="host", port="port")
source_cursor = source_conn.cursor()

# Extract data from the source database
source_cursor.execute("SELECT * FROM table_name")
data = source_cursor.fetchall()

# Connect to the destination database
destination_conn = psycopg2.connect(database="destination_db", user="username", password="password", host="host", port="port")
destination_cursor = destination_conn.cursor()

# Store data in the destination database
for row in data:
    destination_cursor.execute("INSERT INTO destination_table VALUES (%s, %s, %s)", row)

# Commit the changes and close connections
destination_conn.commit()
destination_cursor.close()
destination_conn.close()
source_cursor.close()
source_conn.close()

"""b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.

Answer:
Implementing a real-time data ingestion pipeline for processing sensor data from IoT devices involves handling a continuous stream of data. Here's a high-level design for such a pipeline:

Data ingestion: Set up a data ingestion mechanism that can receive and process the sensor data in real-time. This can be achieved using technologies such as message queues (e.g., Kafka, RabbitMQ) or streaming platforms (e.g., Apache Flink, Apache Spark Streaming).

Data preprocessing: Apply any necessary preprocessing steps to the incoming data. This may involve parsing the data, filtering outliers or noise, and aggregating or enriching the data as needed.

Real-time analytics: Perform real-time analytics on the preprocessed data. This can include calculating statistics, running machine learning models, or triggering actions based on specific conditions.

Data storage: Store the processed data in a suitable storage system based on the requirements. It could be a database, a data lake, or a combination of both.

Here's a simplified code snippet in Python using Apache Kafka and the Kafka-Python library to demonstrate the ingestion pipeline:


"""

from kafka import KafkaConsumer

# Set up Kafka consumer
consumer = KafkaConsumer('sensor_data_topic', bootstrap_servers='kafka_server:9092')

# Process incoming data
for message in consumer:
    sensor_data = message.value
    # Perform preprocessing on the sensor data
    processed_data = preprocess(sensor_data)

    # Perform real-time analytics
    analytics_result = perform_analytics(processed_data)

    # Store processed data in a storage system
    store_data(analytics_result)

"""c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing.

Answer:
Developing a data ingestion pipeline that handles data from different file formats involves parsing and processing the files while ensuring data validation and cleansing. Here's a high-level design for such a pipeline:

File ingestion: Set up a mechanism to ingest files from different sources, such as local file systems or cloud storage platforms.

File format detection: Determine the format of the incoming file (e.g., CSV, JSON, XML) either by examining file extensions or by using libraries that can automatically detect the format.

File parsing: Parse the file based on its format using appropriate libraries or modules. For example, use the csv module in Python for CSV files and json module for JSON files.

Data validation and cleansing: Apply data validation and cleansing operations to ensure data quality. This may include checking for missing or invalid values, removing duplicates, and transforming data into a consistent format.

Store data: Store the processed data in the selected storage system, such as a database, a data lake, or a cloud storage platform.

Here's a simplified code snippet in Python that demonstrates the ingestion pipeline for handling CSV and JSON files:


"""

import csv
import json

def process_csv_file(file_path):
    with open(file_path, 'r') as file:
        csv_reader = csv.DictReader(file)
        for row in csv_reader:
            # Perform data validation and cleansing operations
            processed_row = validate_and_cleanse(row)

            # Store the processed row in the storage system
            store_data(processed_row)

def process_json_file(file_path):
    with open(file_path, 'r') as file:
        json_data = json.load(file)
        for item in json_data:
            # Perform data validation and cleansing operations
            processed_item = validate_and_cleanse(item)

            # Store the processed item in the storage system
            store_data(processed_item)

# Example usage for processing CSV and JSON files
process_csv_file('data.csv')
process_json_file('data.json')

"""2. Model Training:

a. Build a machine learning model to predict customer churn based on a given dataset. Train the model using appropriate algorithms and evaluate its performance.
   b. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, feature scaling, and dimensionality reduction.
   c. Train a deep learning model for image classification using transfer learning and fine-tuning techniques.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Preprocess data and select relevant features
preprocessed_data = preprocess_data(data)
selected_features = select_features(preprocessed_data)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42)

# Train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# Define the pipeline steps
pipeline = Pipeline([
    ('one_hot_encoding', OneHotEncoder()),
    ('feature_scaling', StandardScaler()),
    ('dimensionality_reduction', PCA(n_components=10)),
    ('model', LogisticRegression())
])

# Fit the pipeline on the training data
pipeline.fit(X_train, y_train)

# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, optimizers

# Load pre-trained model without top layer
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze initial layers
for layer in base_model.layers:
    layer.trainable = False

# Add new layers on top of the pre-trained model
model = models.Sequential()
model.add(base_model)
model.add(layers.GlobalAveragePooling2D())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(num_classes, activation='softmax'))

# Data augmentation
train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Compile the model
model.compile(optimizer=optimizers.Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(train_generator, epochs=10, validation_data=val_generator)

# Fine-tuning (optional)
for layer in model.layers[:15]:
    layer.trainable = False
for layer in model.layers[15:]:
    layer.trainable = True

# Compile the model again for fine-tuning
model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Continue training with fine-tuning
model.fit(train_generator, epochs=10, validation_data=val_generator)

"""3. Model Validation:
   a. Implement cross-validation to evaluate the performance of a regression model for predicting housing prices.
   b. Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary classification problem.
   c. Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets.

"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston

# Load the Boston Housing dataset
data = load_boston()
X, y = data.data, data.target

# Create a regression model
regression_model = LinearRegression()

# Perform cross-validation with 5 folds
scores = cross_val_score(regression_model, X, y, cv=5, scoring='neg_mean_squared_error')

# Convert negative mean squared error scores to positive
mse_scores = -scores

# Print the mean and standard deviation of the scores
print("Mean MSE: {:.2f}".format(mse_scores.mean()))
print("Standard Deviation MSE: {:.2f}".format(mse_scores.std()))

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# Generate a binary classification dataset
X, y = make_classification(n_samples=1000, n_features=10, random_state=42)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a logistic regression model
classification_model = LogisticRegression()

# Train the model
classification_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = classification_model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the evaluation metrics
print("Accuracy: {:.2f}".format(accuracy))
print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))
print("F1 Score: {:.2f}".format(f1))

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_classification

# Generate an imbalanced binary classification dataset
X, y = make_classification(n_samples=1000, n_features=10, weights=[0.9, 0.1], random_state=42)

# Create a stratified k-fold object
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Create a decision tree classifier
classification_model = DecisionTreeClassifier()

# Perform stratified k-fold cross-validation
accuracy_scores = []
for train_index, test_index in stratified_kfold.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    classification_model.fit(X_train, y_train)
    y_pred = classification_model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Print the accuracy scores
for fold, accuracy in enumerate(accuracy_scores):
    print("Fold {}: {:.2f}".format(fold+1, accuracy))

# Print the mean and standard deviation of accuracy
print("Mean Accuracy: {:.2f}".format(np.mean(accuracy_scores)))
print("Standard Deviation Accuracy: {:.2f}".format(np.std(accuracy_scores)))

"""4. Deployment Strategy:
   a. Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions.
   b. Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure.
   c. Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time.

a. Deployment Strategy for Real-Time Recommendation Model:

Infrastructure Setup:

Choose a cloud platform such as AWS or Azure to host your deployment infrastructure.
Set up a scalable and reliable computing environment to handle real-time user interactions and recommendations.
Model Deployment:

Package your machine learning model into a container (e.g., Docker) to ensure consistent deployment across different environments.
Deploy the containerized model to a container orchestration platform like Kubernetes for scalability and fault-tolerance.
Real-Time Data Ingestion:

Establish a data pipeline to ingest user interactions in real-time.
Set up data collection mechanisms to capture user actions or events that trigger recommendations.
Ensure the collected data is properly formatted and cleansed before passing it to the model.
Real-Time Recommendation Generation:

Connect the data pipeline to the deployed model.
Implement a real-time recommendation engine that takes user interactions as input and generates recommendations using the deployed model.
Utilize techniques like collaborative filtering, content-based filtering, or deep learning-based approaches to generate personalized recommendations.
API Development:

Expose the recommendation engine as an API to allow external systems or applications to interact with it.
Define appropriate API endpoints for receiving user interactions and returning real-time recommendations.
Load Testing and Scalability:

Perform load testing to ensure the system can handle the expected user traffic.
Use auto-scaling mechanisms provided by the cloud platform to automatically scale the infrastructure based on the incoming traffic.
b. Deployment Pipeline for Machine Learning Models:

Version Control and Collaboration:

Utilize a version control system like Git to manage the source code, including the machine learning model code and associated scripts.
Establish a collaborative workflow for multiple team members to contribute to the development and deployment process.
Continuous Integration (CI):

Set up a CI system such as Jenkins, Travis CI, or GitLab CI/CD to automatically build and test the model code whenever changes are pushed to the repository.
Configure the CI system to run unit tests, code quality checks, and any other necessary validations.
Model Training and Evaluation:

Design a process to train and evaluate the machine learning model using appropriate datasets.
Implement automated scripts or workflows to execute model training and evaluation tasks.
Model Packaging and Artifact Management:

Create a packaging mechanism to bundle the trained model, dependencies, and associated files into a deployable artifact.
Utilize artifact management tools like Nexus or Artifactory to store and organize the model artifacts.
Deployment Automation:

Use infrastructure-as-code tools like Terraform or CloudFormation to define and provision the deployment infrastructure on cloud platforms (e.g., AWS, Azure).
Automate the deployment process using deployment scripts or configuration management tools like Ansible or Chef.
Testing and Validation:

Implement automated tests to verify the functionality and performance of the deployed model.
Perform integration testing to ensure the model is functioning correctly within the deployment environment.
Continuous Deployment (CD):

Configure the CI system to trigger the deployment pipeline automatically after successful testing and validation.
Deploy the packaged model artifact to the target cloud platform (e.g., AWS, Azure) using the defined infrastructure-as-code templates.
c. Monitoring and Maintenance Strategy for Deployed Models:

Logging and Metrics:

Implement logging mechanisms to capture relevant system and application logs.
Define and monitor key performance metrics such as response time, error rates, and throughput to assess the model's performance.
Alerting and Notifications:

Set up alerts and notifications to proactively detect anomalies, errors, or performance degradation.
Utilize monitoring tools like Prometheus, Datadog, or CloudWatch to track metrics and send alerts.
Error Handling and Retraining:

Develop error handling mechanisms to gracefully handle failures or errors encountered during model execution.
Implement retraining pipelines to periodically update and improve the model based on new data.
Regular Model Evaluation:

Set up regular model evaluation processes to assess the model's accuracy, precision, recall, or other relevant metrics.
Compare the model's performance against the predefined thresholds and trigger retraining or fine-tuning if necessary.
Security and Access Control:

Ensure appropriate security measures are in place to protect the deployed model and associated data.
Implement access controls and permissions to restrict unauthorized access to the model or sensitive information.
Regular Infrastructure Updates:

Stay updated with security patches, bug fixes, and new feature releases for the underlying infrastructure, frameworks, and dependencies.
Schedule regular maintenance windows to apply updates and ensure the system remains secure and reliable.
"""